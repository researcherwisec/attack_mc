{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a378348",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6840,
     "status": "ok",
     "timestamp": 1648438388783,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "3a378348",
    "outputId": "c6498c63-a66e-4ff4-dd17-908886d09ff9"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Reshape, LSTM, Conv1D, MaxPooling1D, Conv2D, Input, Concatenate, Add, Embedding\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "#from keras.layers.advanced_activations import ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d0439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os,random\n",
    "from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zXsyD8Tep5pL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1631,
     "status": "ok",
     "timestamp": 1648438390402,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "zXsyD8Tep5pL",
    "outputId": "a010ed1c-982b-4070-c1cc-8cfe98bc80e5"
   },
   "outputs": [],
   "source": [
    "NB_CLASSES = 4 # number of outputs = number of classes\n",
    "VERBOSE = 1\n",
    "BATCH_SIZE = 10\n",
    "NB_EPOCH = 30\n",
    "LENGTH = 500\n",
    "target_names = ['Non-MO', 'MO']\n",
    "file_short = \"D:/BMO/Main/savedModels/_short.h5\"\n",
    "file_long = \"D:/BMO/Main/savedModels/_long.h5\"\n",
    "file_best = \"D:/BMO/Main/savedModels/_best.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd91f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Loading MO (2-state) dataset..........\n",
      "df_mo Shape =  (56000, 2049)\n",
      "CPU times: total: 35.6 s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\".........Loading MO (2-state) dataset..........\")\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/BPSK-2/real.xls',sep='\\t', header=None) \n",
    "im2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/BPSK-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/QPSK-2/real.xls',sep='\\t', header=None) \n",
    "im4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/QPSK-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/16-QAM-2/real.xls',sep='\\t', header=None) \n",
    "im16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/16-QAM-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/64-QAM-2/real.xls',sep='\\t', header=None) \n",
    "im64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/64-QAM-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "#merge two dataframes into one\n",
    "df2psk_2st = re2psk_2st.append(im2psk_2st)\n",
    "df2psk_2st['Mod'] = 0 # 0 = BPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df4psk_2st = re4psk_2st.append(im4psk_2st)\n",
    "df4psk_2st['Mod'] = 1 # 1 = QPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df16qam_2st = re16qam_2st.append(im16qam_2st)\n",
    "df16qam_2st['Mod'] = 2 # 2 = 16-QAM 2-state\n",
    "#merge two dataframes into one\n",
    "df64qam_2st = re64qam_2st.append(im64qam_2st)\n",
    "df64qam_2st['Mod'] = 3 # 3 = 64-QAM 2-state\n",
    "\n",
    "# combine all\n",
    "df_2st = df2psk_2st.append(df4psk_2st)\n",
    "df_2st = df_2st.append(df16qam_2st)\n",
    "df_2st = df_2st.append(df64qam_2st)\n",
    "\n",
    "#df_mo = df_2st\n",
    "#df_mo = df_mo.sample(frac = 1)\n",
    "print('df_mo Shape = ', df_2st.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89de8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Combining all data..........\n",
      "df_all Shape =  (56000, 2049)\n",
      "..........Shuffling done..........\n",
      "df_all Size =  13080816\n",
      "Dimension =  2\n",
      "Combined Shape =  (6384, 2049)\n"
     ]
    }
   ],
   "source": [
    "print(\"..........Combining all data..........\")\n",
    "\n",
    "# combine all\n",
    "#df = df_mo.append(df_nonmo)\n",
    "df_all = df_2st.sample(frac = 1)\n",
    "print('df_all Shape = ', df_all.shape)\n",
    "print(\"..........Shuffling done..........\")\n",
    "\n",
    "# reducing samples to tune only\n",
    "df_all = df_all.iloc[:6384]\n",
    "print('df_all Size = ', df_all.size)\n",
    "print('Dimension = ', df_all.ndim)\n",
    "print('Combined Shape = ', df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b2b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Training set..........\n",
      "(4596, 500, 1)\n",
      "(4596,)\n",
      "..........Validation set..........\n",
      "(511, 500, 1)\n",
      "(511,)\n",
      "..........Testing set..........\n",
      "(1277, 500, 1)\n",
      "(1277,)\n"
     ]
    }
   ],
   "source": [
    "# Separating X and y\n",
    "y = df_all['Mod'] # 1D targer vector\n",
    "X = df_all.drop(columns='Mod')\n",
    "\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "    \n",
    "X.drop(X.iloc[:, LENGTH:2048], inplace = True, axis = 1)\n",
    "\n",
    "X = np.expand_dims(X, -1)\n",
    "\n",
    "# Split into training/testing sets with 20% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1) \n",
    "\n",
    "print(\"..........Training set..........\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"..........Validation set..........\")\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(\"..........Testing set..........\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Convert class vectors to categorical classes matrices\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "y_val = np_utils.to_categorical(y_val, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99d3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import maxnorm\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "   # model.add(Embedding(input_dim = 188, output_dim = 50, input_length = INPUT_SHAPE))\n",
    "    model.add(LSTM(LENGTH))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(NB_CLASSES, activation='sigmoid'))\n",
    "    \n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(ELU(alpha=1.0, name='activation1'))\n",
    "    #model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
    "    #                           padding='same', name='pool1'))\n",
    "    model.add(Dropout(0))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Flatten(name='flatten1'))\n",
    "    model.add(Dense(NB_CLASSES, kernel_initializer=glorot_uniform(seed=0), name='dense1'))\n",
    "    model.add(Activation('softmax', name=\"softmax\"))\n",
    "    \n",
    "    # Output layer\n",
    "   # model.add(Flatten(name='flatten1'))\n",
    "   # model.add(Dense(NB_CLASSES, kernel_initializer=glorot_uniform(seed=0), name='dense1'))\n",
    "   # model.add(Activation('softmax', name=\"softmax\"))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.01, momentum=0.6), metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e9042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4076 - accuracy: 0.2493\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25049, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 425s 3s/step - loss: 1.4076 - accuracy: 0.2493 - val_loss: 1.3877 - val_accuracy: 0.2505\n",
      "Epoch 2/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4025 - accuracy: 0.2574\n",
      "Epoch 2: val_accuracy improved from 0.25049 to 0.25832, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 424s 3s/step - loss: 1.4025 - accuracy: 0.2574 - val_loss: 1.3878 - val_accuracy: 0.2583\n",
      "Epoch 3/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4022 - accuracy: 0.2517\n",
      "Epoch 3: val_accuracy improved from 0.25832 to 0.29941, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 433s 3s/step - loss: 1.4022 - accuracy: 0.2517 - val_loss: 1.3855 - val_accuracy: 0.2994\n",
      "Epoch 4/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3991 - accuracy: 0.2591\n",
      "Epoch 4: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 443s 3s/step - loss: 1.3991 - accuracy: 0.2591 - val_loss: 1.3995 - val_accuracy: 0.2622\n",
      "Epoch 5/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4008 - accuracy: 0.2633\n",
      "Epoch 5: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 439s 3s/step - loss: 1.4008 - accuracy: 0.2633 - val_loss: 1.4038 - val_accuracy: 0.2505\n",
      "Epoch 6/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4013 - accuracy: 0.2591\n",
      "Epoch 6: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 615s 4s/step - loss: 1.4013 - accuracy: 0.2591 - val_loss: 1.3868 - val_accuracy: 0.2779\n",
      "Epoch 7/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.4000 - accuracy: 0.2576\n",
      "Epoch 7: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 432s 3s/step - loss: 1.4000 - accuracy: 0.2576 - val_loss: 1.3948 - val_accuracy: 0.2642\n",
      "Epoch 8/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3940 - accuracy: 0.2668\n",
      "Epoch 8: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 426s 3s/step - loss: 1.3940 - accuracy: 0.2668 - val_loss: 1.4137 - val_accuracy: 0.2818\n",
      "Epoch 9/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3928 - accuracy: 0.2622\n",
      "Epoch 9: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 435s 3s/step - loss: 1.3928 - accuracy: 0.2622 - val_loss: 1.3997 - val_accuracy: 0.2329\n",
      "Epoch 10/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3940 - accuracy: 0.2678\n",
      "Epoch 10: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 479s 3s/step - loss: 1.3940 - accuracy: 0.2678 - val_loss: 1.4052 - val_accuracy: 0.2896\n",
      "Epoch 11/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3946 - accuracy: 0.2646\n",
      "Epoch 11: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 430s 3s/step - loss: 1.3946 - accuracy: 0.2646 - val_loss: 1.3974 - val_accuracy: 0.2857\n",
      "Epoch 12/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3926 - accuracy: 0.2696\n",
      "Epoch 12: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 431s 3s/step - loss: 1.3926 - accuracy: 0.2696 - val_loss: 1.4004 - val_accuracy: 0.2701\n",
      "Epoch 13/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.3937 - accuracy: 0.2661\n",
      "Epoch 13: val_accuracy did not improve from 0.29941\n",
      "144/144 [==============================] - 422s 3s/step - loss: 1.3937 - accuracy: 0.2661 - val_loss: 1.3885 - val_accuracy: 0.2779\n",
      "Epoch 13: early stopping\n",
      "CPU times: total: 8h 30min 21s\n",
      "Wall time: 1h 37min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lr = create_model()\n",
    "# simple early stopping\n",
    "es_lr = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc_lr = ModelCheckpoint(file_best, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# fit model\n",
    "model_history = model_lr.fit(X_train, y_train, epochs=NB_EPOCH, verbose=VERBOSE, \n",
    "                        validation_data=(X_val, y_val), callbacks=[es_lr, mc_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1677f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 44s 1s/step - loss: 1.3931 - accuracy: 0.2717\n",
      "40/40 [==============================] - 44s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.29      0.30       342\n",
      "           1       0.26      0.11      0.15       310\n",
      "           2       0.29      0.35      0.32       324\n",
      "           3       0.23      0.33      0.27       301\n",
      "\n",
      "    accuracy                           0.27      1277\n",
      "   macro avg       0.27      0.27      0.26      1277\n",
      "weighted avg       0.27      0.27      0.26      1277\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Start evaluating model with testing data\n",
    "score_test = model_lr.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "y_pred = model_lr.predict(X_test)\n",
    "    \n",
    "y_pred1=np.argmax(y_pred, axis=1)\n",
    "y_test1=np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db870d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_hypertune-fixedEncrypt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
