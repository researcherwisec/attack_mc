{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a378348",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6840,
     "status": "ok",
     "timestamp": 1648438388783,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "3a378348",
    "outputId": "c6498c63-a66e-4ff4-dd17-908886d09ff9"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Reshape, LSTM, Conv1D, MaxPooling1D, Conv2D, Input, Concatenate, Add, Embedding\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "#from keras.layers.advanced_activations import ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d0439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os,random\n",
    "from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zXsyD8Tep5pL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1631,
     "status": "ok",
     "timestamp": 1648438390402,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "zXsyD8Tep5pL",
    "outputId": "a010ed1c-982b-4070-c1cc-8cfe98bc80e5"
   },
   "outputs": [],
   "source": [
    "NB_CLASSES = 4 # number of outputs = number of classes\n",
    "VERBOSE = 1\n",
    "BATCH_SIZE = 10\n",
    "NB_EPOCH = 30\n",
    "LENGTH = 500\n",
    "target_names = ['Non-MO', 'MO']\n",
    "file_short = \"D:/BMO/Main/savedModels/_short.h5\"\n",
    "file_long = \"D:/BMO/Main/savedModels/_long.h5\"\n",
    "file_best = \"D:/BMO/Main/savedModels/_best.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd91f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Loading MO (2-state) dataset..........\n",
      "df_mo Shape =  (56000, 2049)\n",
      "CPU times: total: 34.9 s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\".........Loading MO (2-state) dataset..........\")\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/BPSK-2/real.xls',sep='\\t', header=None) \n",
    "im2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/BPSK-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/QPSK-2/real.xls',sep='\\t', header=None) \n",
    "im4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/QPSK-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/16-QAM-2/real.xls',sep='\\t', header=None) \n",
    "im16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/16-QAM-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/64-QAM-2/real.xls',sep='\\t', header=None) \n",
    "im64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/64-QAM-2/im.xls',sep='\\t', header=None)\n",
    "\n",
    "#merge two dataframes into one\n",
    "df2psk_2st = re2psk_2st.append(im2psk_2st)\n",
    "df2psk_2st['Mod'] = 0 # 0 = BPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df4psk_2st = re4psk_2st.append(im4psk_2st)\n",
    "df4psk_2st['Mod'] = 1 # 1 = QPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df16qam_2st = re16qam_2st.append(im16qam_2st)\n",
    "df16qam_2st['Mod'] = 2 # 2 = 16-QAM 2-state\n",
    "#merge two dataframes into one\n",
    "df64qam_2st = re64qam_2st.append(im64qam_2st)\n",
    "df64qam_2st['Mod'] = 3 # 3 = 64-QAM 2-state\n",
    "\n",
    "# combine all\n",
    "df_2st = df2psk_2st.append(df4psk_2st)\n",
    "df_2st = df_2st.append(df16qam_2st)\n",
    "df_2st = df_2st.append(df64qam_2st)\n",
    "\n",
    "#df_mo = df_2st\n",
    "#df_mo = df_mo.sample(frac = 1)\n",
    "print('df_mo Shape = ', df_2st.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89de8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Combining all data..........\n",
      "df_all Shape =  (56000, 2049)\n",
      "..........Shuffling done..........\n",
      "df_all Size =  13080816\n",
      "Dimension =  2\n",
      "Combined Shape =  (6384, 2049)\n"
     ]
    }
   ],
   "source": [
    "print(\"..........Combining all data..........\")\n",
    "\n",
    "# combine all\n",
    "#df = df_mo.append(df_nonmo)\n",
    "df_all = df_2st.sample(frac = 1)\n",
    "print('df_all Shape = ', df_all.shape)\n",
    "print(\"..........Shuffling done..........\")\n",
    "\n",
    "# reducing samples to tune only\n",
    "df_all = df_all.iloc[:6384]\n",
    "print('df_all Size = ', df_all.size)\n",
    "print('Dimension = ', df_all.ndim)\n",
    "print('Combined Shape = ', df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b2b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Training set..........\n",
      "(4596, 500, 1)\n",
      "(4596,)\n",
      "..........Validation set..........\n",
      "(511, 500, 1)\n",
      "(511,)\n",
      "..........Testing set..........\n",
      "(1277, 500, 1)\n",
      "(1277,)\n"
     ]
    }
   ],
   "source": [
    "# Separating X and y\n",
    "y = df_all['Mod'] # 1D targer vector\n",
    "X = df_all.drop(columns='Mod')\n",
    "\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "    \n",
    "X.drop(X.iloc[:, LENGTH:2048], inplace = True, axis = 1)\n",
    "\n",
    "X = np.expand_dims(X, -1)\n",
    "\n",
    "# Split into training/testing sets with 20% split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1) \n",
    "\n",
    "print(\"..........Training set..........\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"..........Validation set..........\")\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(\"..........Testing set..........\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Convert class vectors to categorical classes matrices\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "y_val = np_utils.to_categorical(y_val, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99d3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import maxnorm\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        filter_num = ['None',32,64,128,256]\n",
    "        kernel_size = ['None',8,8,8,8]\n",
    "        conv_stride_size = ['None',1,1,1,1]\n",
    "        pool_stride_size = ['None',4,4,4,4]\n",
    "        pool_size = ['None',8,8,8,8]\n",
    "        batch_size = 10\n",
    "        dropout_rate =0.0\n",
    "        # Feature extraction\n",
    "        model.add(Conv1D(filters=filter_num[1], kernel_size=kernel_size[1], input_shape=INPUT_SHAPE,\n",
    "                             strides=conv_stride_size[1], padding='same',\n",
    "                             name='convolution1'))\n",
    "        model.add(BatchNormalization(axis=-1))\n",
    "        model.add(ELU(alpha=1.0, name='activation1'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size[1], strides=pool_stride_size[1],\n",
    "                                   padding='same', name='pool1'))\n",
    "        model.add(Dropout(dropout_rate, name='dropout1'))\n",
    "        \n",
    "        model.add(BatchNormalization(axis=-1))\n",
    "        model.add(ELU(alpha=1.0, name='activation2'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size[2], strides=pool_stride_size[2],\n",
    "                                   padding='same', name='pool2'))\n",
    "        model.add(Dropout(dropout_rate, name='dropout2'))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Flatten(name='flatten1'))\n",
    "        model.add(Dense(NB_CLASSES, kernel_initializer=glorot_uniform(seed=0), name='dense1'))\n",
    "        model.add(Activation('softmax', name=\"softmax\"))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.01, momentum=0.6), metrics=[\"accuracy\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e9042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "143/144 [============================>.] - ETA: 0s - loss: 0.9596 - accuracy: 0.6532\n",
      "Epoch 1: val_accuracy improved from -inf to 0.57534, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 3s 16ms/step - loss: 0.9575 - accuracy: 0.6540 - val_loss: 1.0356 - val_accuracy: 0.5753\n",
      "Epoch 2/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.4388 - accuracy: 0.7895\n",
      "Epoch 2: val_accuracy improved from 0.57534 to 0.75342, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.4408 - accuracy: 0.7885 - val_loss: 0.4996 - val_accuracy: 0.7534\n",
      "Epoch 3/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.4377 - accuracy: 0.8065\n",
      "Epoch 3: val_accuracy improved from 0.75342 to 0.79452, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.4362 - accuracy: 0.8070 - val_loss: 0.3874 - val_accuracy: 0.7945\n",
      "Epoch 4/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8271\n",
      "Epoch 4: val_accuracy did not improve from 0.79452\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.3591 - accuracy: 0.8264 - val_loss: 0.6168 - val_accuracy: 0.7534\n",
      "Epoch 5/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.8500\n",
      "Epoch 5: val_accuracy improved from 0.79452 to 0.79648, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.3200 - accuracy: 0.8497 - val_loss: 0.4627 - val_accuracy: 0.7965\n",
      "Epoch 6/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8511\n",
      "Epoch 6: val_accuracy did not improve from 0.79648\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.3049 - accuracy: 0.8512 - val_loss: 0.6472 - val_accuracy: 0.7867\n",
      "Epoch 7/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.8593\n",
      "Epoch 7: val_accuracy did not improve from 0.79648\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.2944 - accuracy: 0.8575 - val_loss: 0.4133 - val_accuracy: 0.7789\n",
      "Epoch 8/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.8690\n",
      "Epoch 8: val_accuracy did not improve from 0.79648\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.2810 - accuracy: 0.8690 - val_loss: 0.4097 - val_accuracy: 0.7769\n",
      "Epoch 9/30\n",
      "140/144 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8699\n",
      "Epoch 9: val_accuracy improved from 0.79648 to 0.80626, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.2753 - accuracy: 0.8688 - val_loss: 0.4222 - val_accuracy: 0.8063\n",
      "Epoch 10/30\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.8708\n",
      "Epoch 10: val_accuracy did not improve from 0.80626\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.2701 - accuracy: 0.8708 - val_loss: 0.4527 - val_accuracy: 0.7554\n",
      "Epoch 11/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.8794\n",
      "Epoch 11: val_accuracy did not improve from 0.80626\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.2529 - accuracy: 0.8799 - val_loss: 0.4388 - val_accuracy: 0.7867\n",
      "Epoch 12/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.8927\n",
      "Epoch 12: val_accuracy did not improve from 0.80626\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.2293 - accuracy: 0.8927 - val_loss: 0.4506 - val_accuracy: 0.7750\n",
      "Epoch 13/30\n",
      "141/144 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.8903\n",
      "Epoch 13: val_accuracy did not improve from 0.80626\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.2375 - accuracy: 0.8910 - val_loss: 0.4992 - val_accuracy: 0.8043\n",
      "Epoch 13: early stopping\n",
      "CPU times: total: 2min 7s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lr = create_model()\n",
    "# simple early stopping\n",
    "es_lr = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc_lr = ModelCheckpoint(file_best, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "# fit model\n",
    "model_history = model_lr.fit(X_train, y_train, epochs=NB_EPOCH, verbose=VERBOSE, \n",
    "                        validation_data=(X_val, y_val), callbacks=[es_lr, mc_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1677f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7980\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.38      0.48       314\n",
      "           1       0.58      0.81      0.68       330\n",
      "           2       1.00      1.00      1.00       320\n",
      "           3       1.00      1.00      1.00       313\n",
      "\n",
      "    accuracy                           0.80      1277\n",
      "   macro avg       0.81      0.80      0.79      1277\n",
      "weighted avg       0.81      0.80      0.79      1277\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Start evaluating model with testing data\n",
    "score_test = model_lr.evaluate(X_test, y_test, verbose=VERBOSE)\n",
    "y_pred = model_lr.predict(X_test)\n",
    "    \n",
    "y_pred1=np.argmax(y_pred, axis=1)\n",
    "y_test1=np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Non-MO', 'MO']\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.rcParams[\"figure.figsize\"] = (18,10)\n",
    "plt.rcParams['font.size'] = (22)\n",
    "#cm = confusion_matrix(truth_labels, predicted_labels)\n",
    "cmratio = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cmratio= np.around(cmratio.astype('float') / cmratio.sum(axis=1)[:, np.newaxis], decimals=1)\n",
    "#f = sns.heatmap(cmratio, cmap='YlGnBu', annot=True, square=True, yticklabels = True)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmratio, display_labels=target_names)\n",
    "disp.plot(cmap='YlGnBu', values_format='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db870d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_hypertune-fixedEncrypt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
