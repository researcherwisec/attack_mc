{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a378348",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6840,
     "status": "ok",
     "timestamp": 1648438388783,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "3a378348",
    "outputId": "c6498c63-a66e-4ff4-dd17-908886d09ff9"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Reshape, LSTM, Conv1D, MaxPooling1D, Conv2D, Input, Concatenate, Add\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers.core import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "#from keras.layers.advanced_activations import ELU\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d0439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os,random\n",
    "from tensorflow.keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as Model\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zXsyD8Tep5pL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1631,
     "status": "ok",
     "timestamp": 1648438390402,
     "user": {
      "displayName": "Naureen Hoque",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03798803644117143121"
     },
     "user_tz": 240
    },
    "id": "zXsyD8Tep5pL",
    "outputId": "a010ed1c-982b-4070-c1cc-8cfe98bc80e5"
   },
   "outputs": [],
   "source": [
    "LENGTH = 500\n",
    "NB_CLASSES = 4 # number of outputs = number of classes\n",
    "VERBOSE = 1\n",
    "BATCH_SIZE = 10\n",
    "NB_EPOCH = 10\n",
    "target_names = ['Non-MO', 'MO']\n",
    "file_short = \"D:/BMO/Main/savedModels/_short.h5\"\n",
    "file_long = \"D:/BMO/Main/savedModels/_long.h5\"\n",
    "file_best = \"D:/BMO/Main/savedModels/_best.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a06a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Loading MO (2-state) dataset..........\n",
      "df_2st Shape =  (28000, 2049)\n",
      "CPU times: total: 45.1 s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\".........Loading MO (2-state) dataset..........\")\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/BPSK-2/real_snr00.csv',sep=',', header=None) \n",
    "im2psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/BPSK-2/im_snr00.csv',sep=',', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/QPSK-2/real_snr00.csv',sep=',', header=None) \n",
    "im4psk_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/QPSK-2/im_snr00.csv',sep=',', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/16-QAM-2/real_snr00.csv',sep=',', header=None) \n",
    "im16qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/16-QAM-2/im_snr00.csv',sep=',', header=None)\n",
    "\n",
    "# sep separates each column and header = none means it is going to read from row 1\n",
    "re64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/64-QAM-2/real_snr00.csv',sep=',', header=None) \n",
    "im64qam_2st = pd.read_csv('D:/BMO/Main/bigDataset/SNR-based/64-QAM-2/im_snr00.csv',sep=',', header=None)\n",
    "\n",
    "#merge two dataframes into one\n",
    "df2psk_2st = re2psk_2st + im2psk_2st * 1j\n",
    "df2psk_2st['Mod'] = 0 # 0 = BPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df4psk_2st = re4psk_2st + im4psk_2st * 1j\n",
    "df4psk_2st['Mod'] = 1 # 1 = QPSK 2-state\n",
    "#merge two dataframes into one\n",
    "df16qam_2st = re16qam_2st + im16qam_2st * 1j\n",
    "df16qam_2st['Mod'] = 2 # 2 = 16-QAM 2-state\n",
    "#merge two dataframes into one\n",
    "df64qam_2st = re64qam_2st + im64qam_2st * 1j\n",
    "df64qam_2st['Mod'] = 3 # 3 = 64-QAM 2-state\n",
    "\n",
    "# combine all\n",
    "df_2st = df2psk_2st.append(df4psk_2st)\n",
    "df_2st = df_2st.append(df16qam_2st)\n",
    "df_2st = df_2st.append(df64qam_2st)\n",
    "\n",
    "df_2st = df_2st.sample(frac = 1)\n",
    "#df_2st = df_2st.iloc[:5000]\n",
    "print('df_2st Shape = ', df_2st.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89de8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Combining all data..........\n",
      "df_all Shape =  (28000, 2049)\n",
      "..........Shuffling done..........\n",
      "df_all Size =  13080816\n",
      "Dimension =  2\n",
      "Combined Shape =  (6384, 2049)\n"
     ]
    }
   ],
   "source": [
    "print(\"..........Combining all data..........\")\n",
    "\n",
    "# combine all\n",
    "#df_all = dfmc.append(df_2st)\n",
    "df_all = df_2st.sample(frac = 1)\n",
    "print('df_all Shape = ', df_all.shape)\n",
    "print(\"..........Shuffling done..........\")\n",
    "\n",
    "# reducing samples to tune only\n",
    "df_all = df_all.iloc[:6384]\n",
    "print('df_all Size = ', df_all.size)\n",
    "print('Dimension = ', df_all.ndim)\n",
    "print('Combined Shape = ', df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720eef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6384,)\n",
      "X_train: (5107, 500, 1)\n",
      "Y_train: (5107, 4)\n",
      "X_test: (1277, 500, 1)\n",
      "Y_test: (1277, 4)\n"
     ]
    }
   ],
   "source": [
    "# Separating X and y\n",
    "Y = df_all['Mod'] # 1D targer vector\n",
    "X = df_all.drop(columns='Mod')\n",
    "print(Y.shape)\n",
    "Y = np_utils.to_categorical(Y, NB_CLASSES)\n",
    "INPUT_SHAPE = (LENGTH,1)\n",
    "    \n",
    "X.drop(X.iloc[:, LENGTH:2048], inplace = True, axis = 1)\n",
    "\n",
    "X = np.expand_dims(X, -1)\n",
    "\n",
    "n_examples = X.shape[0]\n",
    "# n_train = int(n_examples * 0.9877) \n",
    "n_train = int(n_examples * 0.8)  \n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)  #Randomly select training sample subscript\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx)) #Test sample index\n",
    "#print(train_idx, test_idx)\n",
    "X_train = X[train_idx]  #training samples\n",
    "X_test =  X[test_idx]  #testing samples\n",
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]\n",
    "print(\"X_train:\",X_train.shape)\n",
    "print(\"Y_train:\",Y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"Y_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2db486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1:])\n",
    "# Input(in_shp)\n",
    "classes = target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36325067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500, 1)]          0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 500)            0         \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, None, 500)        2005500   \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 500)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " alpha_dropout (AlphaDropout  (None, 500)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 2004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,007,504\n",
      "Trainable params: 2,007,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights \n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"ELU\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "#         cfg.update({\n",
    "#             'att': self.MultiHeadSelfAttention(1024, 128),\n",
    "#             'ffn': self.tf.keras.Sequential(\n",
    "#             [layers.Dense(256, activation=\"relu\"), layers.Dense(1024),]\n",
    "#         ),\n",
    "#             'layernorm1': self.layers.LayerNormalization(epsilon=1e-6),\n",
    "#             'layernorm2': self.layers.LayerNormalization(epsilon=1e-6),\n",
    "#             'dropout1': self.Dropout(0.2),\n",
    "#             'dropout2': self.Dropout(0.2),\n",
    "#         })\n",
    "        return cfg    \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "def proposed_model(X_train,classes):\n",
    "\n",
    "    embed_dim = LENGTH  # Embedding size for each token\n",
    "    num_heads = 500  # Number of attention heads\n",
    "    ff_dim = 1000  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#     inputs = layers.Input(shape=(maxlen,))\n",
    "#     embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "#     x = embedding_layer(inputs)\n",
    "#     mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\",\"/gpu:2\"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "#     with mirrored_strategy.scope():\n",
    "\n",
    "    in_shp = X_train.shape[1:]   #Dimensions of each sample\n",
    "\n",
    "    #input layer\n",
    "    X_input = Input(in_shp)\n",
    "    X = Reshape([1,LENGTH], input_shape=in_shp)(X_input)\n",
    "\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(X)\n",
    "    #x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.AlphaDropout(0.3)(x)\n",
    "    x = layers.Dense(NB_CLASSES, activation=\"softmax\")(x)\n",
    "    return tf.keras.models.Model(inputs=X_input, outputs=x)     \n",
    "    \n",
    "model = proposed_model(X_train,classes)    \n",
    "model.compile(loss='categorical_crossentropy', optimizer=tfa.optimizers.LazyAdam(),metrics=['accuracy'])\n",
    "#model.load_weights('D:\\BMO\\Main\\trafo_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0203251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "458/460 [============================>.] - ETA: 0s - loss: 1.8829 - accuracy: 0.2712\n",
      "Epoch 1: val_loss improved from inf to 1.64381, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "460/460 [==============================] - 9s 13ms/step - loss: 1.8819 - accuracy: 0.2715 - val_loss: 1.6438 - val_accuracy: 0.2779\n",
      "Epoch 2/10\n",
      "459/460 [============================>.] - ETA: 0s - loss: 1.4255 - accuracy: 0.3381\n",
      "Epoch 2: val_loss improved from 1.64381 to 1.58086, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.4257 - accuracy: 0.3383 - val_loss: 1.5809 - val_accuracy: 0.3209\n",
      "Epoch 3/10\n",
      "458/460 [============================>.] - ETA: 0s - loss: 1.3314 - accuracy: 0.3930\n",
      "Epoch 3: val_loss improved from 1.58086 to 1.54332, saving model to D:/BMO/Main/savedModels\\_best.h5\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.3319 - accuracy: 0.3925 - val_loss: 1.5433 - val_accuracy: 0.3190\n",
      "Epoch 4/10\n",
      "459/460 [============================>.] - ETA: 0s - loss: 1.2790 - accuracy: 0.4144\n",
      "Epoch 4: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.2790 - accuracy: 0.4143 - val_loss: 1.6732 - val_accuracy: 0.3366\n",
      "Epoch 5/10\n",
      "456/460 [============================>.] - ETA: 0s - loss: 1.2523 - accuracy: 0.4309\n",
      "Epoch 5: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.2532 - accuracy: 0.4302 - val_loss: 1.6559 - val_accuracy: 0.3346\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - ETA: 0s - loss: 1.2205 - accuracy: 0.4521\n",
      "Epoch 6: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.2205 - accuracy: 0.4521 - val_loss: 1.7207 - val_accuracy: 0.3288\n",
      "Epoch 7/10\n",
      "459/460 [============================>.] - ETA: 0s - loss: 1.1820 - accuracy: 0.4719\n",
      "Epoch 7: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.1821 - accuracy: 0.4717 - val_loss: 1.9124 - val_accuracy: 0.3425\n",
      "Epoch 8/10\n",
      "456/460 [============================>.] - ETA: 0s - loss: 1.1512 - accuracy: 0.4853\n",
      "Epoch 8: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.1528 - accuracy: 0.4848 - val_loss: 1.9180 - val_accuracy: 0.3464\n",
      "Epoch 9/10\n",
      "459/460 [============================>.] - ETA: 0s - loss: 1.1109 - accuracy: 0.5083\n",
      "Epoch 9: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.1115 - accuracy: 0.5078 - val_loss: 2.0059 - val_accuracy: 0.3229\n",
      "Epoch 10/10\n",
      "455/460 [============================>.] - ETA: 0s - loss: 1.0398 - accuracy: 0.5473\n",
      "Epoch 10: val_loss did not improve from 1.54332\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 1.0410 - accuracy: 0.5472 - val_loss: 2.3326 - val_accuracy: 0.3190\n",
      "CPU times: total: 3min 42s\n",
      "Wall time: 53.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filepath = \"D:/BMO/Main/savedModels/_best.h5\"\n",
    "history = model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=10,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "#     validation_data=(X_test, Y_test),\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee697fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.16      0.19       290\n",
      "           1       0.33      0.36      0.34       337\n",
      "           2       0.31      0.36      0.33       309\n",
      "           3       0.37      0.41      0.39       341\n",
      "\n",
      "    accuracy                           0.33      1277\n",
      "   macro avg       0.32      0.32      0.32      1277\n",
      "weighted avg       0.32      0.33      0.32      1277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "    \n",
    "Y_pred1=np.argmax(Y_pred, axis=1)\n",
    "Y_test1=np.argmax(Y_test, axis=1)\n",
    "print(classification_report(Y_test1,Y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb9b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_hypertune-fixedEncrypt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
